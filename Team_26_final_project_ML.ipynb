{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SixoZtwPYJHi"
      },
      "source": [
        "# Time-Series Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIsK4Crx0MGj"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn==1.0.2\n",
        "!pip install xgboost\n",
        "!pip install scikit-learn-intelex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQVqVaiD0QDy"
      },
      "source": [
        "### Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H06WMoIY0PhC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import dataset\n",
        "data = pd.read_csv('energydata_complete.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3CTlVgz0dBh"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwWeI3S-0aJb"
      },
      "outputs": [],
      "source": [
        "# Provide information about the dataset\n",
        "# Prints the head (5 first rows) and tail (5 last rows)\n",
        "# and also prints the size of the dataset.\n",
        "def dump_source_info(dataset):\n",
        "  print(dataset.head())\n",
        "  print(\"-----------------------\")\n",
        "  print(dataset.tail())\n",
        "  print(\"-----------------------\")\n",
        "  print(dataset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmqVCOFY0nOP"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVY9hA310q2G"
      },
      "outputs": [],
      "source": [
        "# Prints all the different columns\n",
        "# in the dataset and information\n",
        "# about these columns.\n",
        "def dump_variables(dataset):\n",
        "  print(dataset.columns)\n",
        "  print(\"-----------------------\")\n",
        "  print(dataset.info())\n",
        "  print(\"-----------------------\")\n",
        "  print(dataset.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJyexIf10r2O"
      },
      "source": [
        "### Time granularity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VokyZSyC0-0E"
      },
      "outputs": [],
      "source": [
        "print(data['date'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqoVmn2S2flX"
      },
      "source": [
        "By printing the date column, we observe that the sampling rate is done every 10 minutes for this dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1GATxYh21G9"
      },
      "source": [
        "### Duration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBnMvUY123bk"
      },
      "outputs": [],
      "source": [
        "# We calculate the duration by substracting from the maximum time\n",
        "# index the minimum time index.\n",
        "def dump_duration(dataset):\n",
        "  data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "  # Get the minimum timestamp\n",
        "  min_timestamp = data['date'].min()\n",
        "\n",
        "  # Get the maximum timestamp\n",
        "  max_timestamp = data['date'].max()\n",
        "\n",
        "  # Calculate the duration\n",
        "  duration = max_timestamp - min_timestamp\n",
        "\n",
        "  # Print the duration\n",
        "  print(duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nDh2uGV37kG"
      },
      "outputs": [],
      "source": [
        "dump_duration(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StaA16uQ4H0T"
      },
      "source": [
        "**The duration is 137 days approximately 4.5 months.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8KXpM7i4Ags"
      },
      "source": [
        "### Plotting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyUlcq-q4dgB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the target variable\n",
        "# We set it to be the column\n",
        "# Appliances.\n",
        "def plot_appliances(dataset):\n",
        "  # Reset index and keep 'date' as a column\n",
        "  series = dataset.reset_index()\n",
        "\n",
        "  # Ensure 'date' is in datetime format\n",
        "  series['date'] = pd.to_datetime(series['date'])\n",
        "\n",
        "  plt.rcParams['figure.figsize'] = [12, 6]\n",
        "  sns.set_theme(style='darkgrid')\n",
        "\n",
        "  # Use 'date' column as x-axis\n",
        "  sns.lineplot(data=series, x='date', y='Appliances')\n",
        "\n",
        "  plt.ylabel('Appliances')\n",
        "  plt.xlabel('date')\n",
        "  plt.title('Applicancies time series')\n",
        "\n",
        "  plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek4mUSMJ4hQf"
      },
      "outputs": [],
      "source": [
        "plot_appliances(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1goQTMsw5Exc"
      },
      "source": [
        "### Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3yz10Vy5HuL"
      },
      "outputs": [],
      "source": [
        "# Function that prints if there are any Nan values\n",
        "# Also prints the count and where they are, if there\n",
        "# are nan values.\n",
        "def check_missing_values(series):\n",
        "  if series.isnull().values.any():\n",
        "    print(\"Are there any Nan values? \", series.isnull().values.any())\n",
        "    print(\"The sum of the Nan values at each feature is\\n:\", series.isnull().sum())\n",
        "    nan_check_df = series.isnull()\n",
        "    print(f\"Nan values: \\n {nan_check_df}\")\n",
        "  else:\n",
        "    print(\"Are there any Nan values? \", series.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3nuvzGc5PHL"
      },
      "outputs": [],
      "source": [
        "check_missing_values(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4a16CBg5_r-"
      },
      "source": [
        "**We observe that the original data has no nan values.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXoofJkg6IQM"
      },
      "source": [
        "### Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Ea8AzTZkkz"
      },
      "source": [
        "For outliers the IQR method is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxfgvdhMws2I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFs6DwZepOpL"
      },
      "outputs": [],
      "source": [
        "# Function to replace outliers using IQR method\n",
        "# and print the number of replaced outliers.\n",
        "def replace_outliers_with_threshold(df, threshold=1):\n",
        "    df_cleaned = df.copy()\n",
        "    outliers_replaced = {}\n",
        "\n",
        "    for col in df_cleaned.select_dtypes(include=[np.number]).columns:\n",
        "        Q1 = df_cleaned[col].quantile(0.25)\n",
        "        Q3 = df_cleaned[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - (threshold * IQR)\n",
        "        upper_bound = Q3 + (threshold * IQR)\n",
        "\n",
        "        # Count the number of outliers before replacement\n",
        "        outliers_before = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
        "\n",
        "        # Replace outliers with the threshold values (lower and upper bounds)\n",
        "        df_cleaned[col] = np.where(df_cleaned[col] < lower_bound, lower_bound,\n",
        "                                   np.where(df_cleaned[col] > upper_bound, upper_bound, df_cleaned[col]))\n",
        "\n",
        "        # Count the number of outliers after replacement (should be zero)\n",
        "        outliers_after = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
        "\n",
        "        # Store the number of outliers replaced for this column\n",
        "        outliers_replaced[col] = outliers_before\n",
        "\n",
        "        if outliers_before > 0:\n",
        "            print(f\"Column '{col}': {outliers_before} outliers replaced.\")\n",
        "\n",
        "    return df_cleaned, outliers_replaced\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSYw_mtUISLs"
      },
      "source": [
        "### ACF/PACF Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BbGvhTuIWQT"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# ACF/PACF plot to check if after the decomposition\n",
        "# some columns still have seasonality/trending issues.\n",
        "def plot_acf_pacf(dataset, parameter, lags):\n",
        "\n",
        "  for column in dataset.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    if parameter == 'acf' and column == 'Appliances':\n",
        "      plot_acf(dataset[column], lags=lags)\n",
        "      plt.title(f\"{column} {parameter}\")\n",
        "      plt.xlabel(\"Lag\")\n",
        "      plt.ylabel(\"Correlation\")\n",
        "      plt.legend()\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.show()\n",
        "    elif parameter == 'pacf':\n",
        "      plot_pacf(dataset[column], lags=lags)\n",
        "      plt.title(f\"{column} {parameter}\")\n",
        "      plt.xlabel(\"Lag\")\n",
        "      plt.ylabel(\"Correlation\")\n",
        "      plt.legend()\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WZOq4NOIoew"
      },
      "outputs": [],
      "source": [
        "plot_acf_pacf(data, 'acf', 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgVXOLq-JLSU"
      },
      "source": [
        "from this plot we decide to add lags every :\n",
        "- day (144)\n",
        "- 2 days (288)\n",
        "- 3 days (432)\n",
        "- 12 hours (72)\n",
        "- 6 hours (36)\n",
        "- 3 hours (18)\n",
        "- 2 hours (12)\n",
        "- 1 hour (6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uERbzJ4Jx-Q"
      },
      "source": [
        "we also decide to add rolling averages every:\n",
        "- Approximately every 2 hours (10)\n",
        "- Every 5 hours (30)\n",
        "- Approximately every 8 hours (50)\n",
        "- Every day (144)\n",
        "- Every 2 days (288)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBraGSBn6vuY"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXjmXP9ZZ9TX"
      },
      "outputs": [],
      "source": [
        "# Function that adds lags as a column in the dataset\n",
        "def add_periodic_lags(df, target_column, period=144):\n",
        "    df_lagged = df.copy()\n",
        "    df_lagged[f'{target_column}_lag_{period}'] = df_lagged[target_column].shift(period)\n",
        "\n",
        "    return df_lagged\n",
        "\n",
        "# Function that adds rolling averages in the dataset\n",
        "def add_rolling_means(df, target_column, window_size=144):\n",
        "    df_rolling = df.copy()\n",
        "    df_rolling[f'{target_column}_rolling_mean_{window_size}'] = df_rolling[target_column].rolling(window=window_size).mean()\n",
        "\n",
        "    return df_rolling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wUkeGkYpaXk"
      },
      "outputs": [],
      "source": [
        "# Convert date column to datetime\n",
        "data['date'] = pd.to_datetime(data['date'])\n",
        "\n",
        "# Create NSM feature (Seconds from Midnight)\n",
        "data['NSM'] = data['date'].dt.hour * 3600 + data['date'].dt.minute * 60 + data['date'].dt.second\n",
        "\n",
        "# Create week_status feature (1 for weekday, 0 for weekend)\n",
        "data['week_status'] = data['date'].dt.weekday.apply(lambda x: 1 if x < 5 else 0)\n",
        "\n",
        "# One-hot encode day of the week(Monday to Sunday)\n",
        "day_of_week_encoded = pd.get_dummies(data['date'].dt.day_name(), prefix='day')\n",
        "\n",
        "# Combine the one-hot encoded columns with the original dataset\n",
        "data = pd.concat([data, day_of_week_encoded], axis=1)\n",
        "\n",
        "\n",
        "# Features as per the paper plus new features\n",
        "features = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9',\n",
        "    'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'NSM', 'week_status', 'rv1', 'rv2',\n",
        "] + list(day_of_week_encoded.columns)\n",
        "\n",
        "# Add periodic lags and rolling means on the dataset\n",
        "data_new = add_periodic_lags(data, 'Appliances', 144)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 288)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 432)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 72)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 36)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 18)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 12)\n",
        "data_new = add_periodic_lags(data_new, 'Appliances', 6)\n",
        "data_new = add_rolling_means(data_new, 'Appliances', 10)\n",
        "data_new = add_rolling_means(data_new, 'Appliances', 30)\n",
        "data_new = add_rolling_means(data_new, 'Appliances', 50)\n",
        "data_new = add_rolling_means(data_new, 'Appliances', 144)\n",
        "data_new = add_rolling_means(data_new, 'Appliances', 288)\n",
        "\n",
        "# Create a list with all the columns removing the random variables rv1 and rv2.\n",
        "new_features = [\n",
        "    'lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4',\n",
        "    'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9',\n",
        "    'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility',\n",
        "    'Tdewpoint', 'NSM', 'week_status',\n",
        "    'Appliances_lag_144',\n",
        "       'Appliances_lag_288', 'Appliances_lag_432', 'Appliances_lag_72',\n",
        "       'Appliances_lag_36', 'Appliances_lag_18', 'Appliances_rolling_mean_10',\n",
        "       'Appliances_rolling_mean_30', 'Appliances_rolling_mean_50',\n",
        "       'Appliances_rolling_mean_144', 'Appliances_rolling_mean_288','Appliances_lag_6'\n",
        "       ,'Appliances_lag_12'\n",
        "] + list(day_of_week_encoded.columns)\n",
        "\n",
        "\n",
        "\n",
        "target = 'Appliances'\n",
        "\n",
        "# Drop irrelevant columns\n",
        "# X = data[features]\n",
        "X = data_new[new_features]\n",
        "y = data[target]\n",
        "\n",
        "# Because of the addition of lag features and rolling averages\n",
        "# we decide to fill the nan values with the mean\n",
        "X.fillna(X.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLQPwbbKALzr"
      },
      "source": [
        "From reading the paper we decide to remove the lights Visibility and rv1, rv2 columns. The last two (rv1, rv2) are just random variables and therefore do not contribute to the accuracy of the models. Where we as well shaw that after removing the lights and Visibility got better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06qQ8qfnCcc_"
      },
      "outputs": [],
      "source": [
        "data_new = data_new.drop(columns=['lights', 'Visibility', 'rv1', 'rv2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4SLAkGTCwlA"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame as a CSV file\n",
        "data_new.to_csv('energydata_complete_new.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE1rw5J8phEN"
      },
      "outputs": [],
      "source": [
        "check_missing_values(data_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX9hy3kN2xP1"
      },
      "outputs": [],
      "source": [
        "# Handle missing values after finding out they exist\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Handle outliers using the function and print the count of outliers replaced\n",
        "data_cleaned, outliers_replaced = replace_outliers_with_threshold(data_new)\n",
        "\n",
        "# Prepare the new data for the columns we removed\n",
        "new_features = data_new.columns.tolist()\n",
        "new_features.remove('Appliances')\n",
        "\n",
        "# Initialize the scaler \n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "scaler = MinMaxScaler()  \n",
        "\n",
        "# Get the new X and y for the outlier-removed dataset\n",
        "X_cleaned = data_cleaned[new_features]\n",
        "X_cleaned = X_cleaned.drop(columns=['date'])\n",
        "y_cleaned = data_cleaned[target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TRPZOClEFzG"
      },
      "source": [
        "### Method to plot real vs predicted values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oamGf6lbD67D"
      },
      "outputs": [],
      "source": [
        "def plot_forecast_vs_actual(y_true, y_pred, title=\"Forecasted vs Actual Values\", num_samples=200):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_true.values[:num_samples], label=\"Actual\", color='purple')\n",
        "    plt.plot(y_pred[:num_samples], label=\"Forecasted\", color='green')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(\"Appliance Energy Consumption\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4fD9W_XRflG"
      },
      "outputs": [],
      "source": [
        "def plot_forecast_vs_actual_dl(y_true, y_pred, title=\"Forecasted vs Actual Values\", num_samples=200):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_true[:num_samples], label=\"Actual\", color='purple')\n",
        "    plt.plot(y_pred[:num_samples], label=\"Forecasted\", color='green')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(\"Appliance Energy Consumption\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of_sxB2LEP-Z"
      },
      "source": [
        "### Method to plot the residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mzlb54jjDhfB"
      },
      "outputs": [],
      "source": [
        "def plot_residuals(y_true, y_pred, title=\"Residuals Plot\"):\n",
        "    residuals = y_true - y_pred\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(range(len(residuals)), residuals, color='orange', alpha=0.6)\n",
        "    plt.axhline(0, color='black', linestyle='--', linewidth=2)  # Horizontal line at 0 for reference\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Sample Index\")\n",
        "    plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvaZ-biLOD-"
      },
      "source": [
        "### Evaluate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiYk_NRALLbl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "def evaluate_metrics(y_train, y_pred_train, y_test, y_pred_test):\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    test_r2 = r2_score(y_test, y_pred_test)\n",
        "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "    train_mape = mean_absolute_percentage_error(y_train, y_pred_train)\n",
        "    test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
        "\n",
        "    print(\"-------------------------------------------------------------------------------\")\n",
        "    print(f\"Training RMSE: {train_rmse}, Training R^2: {train_r2}\")\n",
        "    print(f\"Testing RMSE: {test_rmse}, Testing R^2: {test_r2}\")\n",
        "    print(f\"Training MAE: {train_mae}, Testing MAE: {test_mae}\")\n",
        "    print(f\"Training MAPE: {train_mape * 100:.2f}%, Testing MAPE: {test_mape * 100:.2f}%\")\n",
        "    print(\"-------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZu3N3n_TsO_"
      },
      "outputs": [],
      "source": [
        "def evaluate_metrics_naive(y_test, y_pred):\n",
        "  rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  mape = mean_absolute_percentage_error(y_test, y_pred)*100\n",
        "\n",
        "  print(f\"RMSE: {rmse}\")\n",
        "  print(f\"R-squared: {r2}\")\n",
        "  print(f\"MAE: {mae}\")\n",
        "  print(f\"MAPE: {mape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6xrrh8y7w_"
      },
      "source": [
        "## **6.1 Baseline Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-LMdtN3zI5w"
      },
      "source": [
        "###  **Naïve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPyHD2kHzwmE"
      },
      "outputs": [],
      "source": [
        "# Naive model\n",
        "class NaiveForecast:\n",
        "  def __init__(self):\n",
        "    self.last_value = None  # Store the last observed value for forecasting\n",
        "\n",
        "  def fit(self, series):\n",
        "    self.last_value = series.iloc[-1]  # The last value of the series\n",
        "\n",
        "  def predict(self, n_periods=1):\n",
        "    if self.last_value is None:\n",
        "        raise ValueError(\"Model is not fitted yet. Call fit() before predict().\")\n",
        "    return [self.last_value] * n_periods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlGbz7tmKxyY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def pred_naive(y_train, y_test):\n",
        "  start_time = time.time()\n",
        "  naive_model = NaiveForecast()\n",
        "  naive_model.fit(y_train)\n",
        "\n",
        "  naive_predictions = naive_model.predict(n_periods=len(y_test))\n",
        "  name = \"Naive\"\n",
        "  end_time = time.time()\n",
        "  execution_time = end_time - start_time\n",
        "  return naive_predictions, name, execution_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pisBhKyTChd"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = train_test_split(y_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions using the Naive model\n",
        "naive_predictions, model_name, execution_time = pred_naive(y_train, y_test)\n",
        "\n",
        "evaluate_metrics_naive(y_test, naive_predictions)\n",
        "print(f'Execution time of the naive model is: {execution_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwpNcjYbUINE"
      },
      "outputs": [],
      "source": [
        "plot_forecast_vs_actual(y_test, naive_predictions, title=\"Forecasted vs Actual Values\", num_samples=200)\n",
        "\n",
        "plot_residuals(y_test, naive_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449jxuiozPCA"
      },
      "source": [
        "### **Moving Average**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTOTCCjpzw2i"
      },
      "outputs": [],
      "source": [
        "class MovingAverageForecast:\n",
        "  def __init__(self, window_size):\n",
        "    self.window_size = window_size\n",
        "    self.history = None\n",
        "\n",
        "  def fit(self, series):\n",
        "    self.history = series.tolist() #.iloc[-self.window_size:]\n",
        "\n",
        "  def predict(self, n_periods=1):\n",
        "    if self.history is None:\n",
        "        raise ValueError(\"Model is not fitted yet. Call fit() before predict().\")\n",
        "    predictions = []\n",
        "    temp_history = self.history.copy()  # Create a copy of the history to simulate rolling predictions\n",
        "    for _ in range(n_periods):\n",
        "        if len(temp_history) < self.window_size:\n",
        "            raise ValueError(\"Not enough history to calculate moving average.\")\n",
        "        # Compute the moving average of the last `window_size` values\n",
        "        prediction = np.mean(temp_history[-self.window_size:])\n",
        "        predictions.append(prediction)\n",
        "        temp_history.append(prediction)  # Update the history for rolling predictions\n",
        "    return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWWaJRNyK0YZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def pred_moving_average_grid_search(y_train, y_test, window_sizes=[1, 3, 5, 7, 10, 14, 21, 30]):\n",
        "    best_window_size = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for window_size in window_sizes:\n",
        "        moving_avg_model = MovingAverageForecast(window_size=window_size)\n",
        "        moving_avg_model.fit(y_train)\n",
        "        predictions = moving_avg_model.predict(n_periods=len(y_test))\n",
        "\n",
        "        score = mean_squared_error(y_test, predictions)\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_window_size = window_size\n",
        "\n",
        "    start_time = time.time()\n",
        "    final_model = MovingAverageForecast(window_size=best_window_size)\n",
        "    final_model.fit(y_train)\n",
        "    final_predictions = final_model.predict(n_periods=len(y_test))\n",
        "    end_time = time.time()\n",
        "\n",
        "    name = f\"Moving Average (Best Window Size: {best_window_size})\"\n",
        "    execution_time = end_time - start_time\n",
        "    return final_predictions, name, execution_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrlWbD1oU5eg"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = train_test_split(y_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions using the moving avg model\n",
        "ma_predictions, model_name, execution_time = pred_moving_average_grid_search(y_train, y_test)\n",
        "\n",
        "evaluate_metrics_naive(y_test, ma_predictions)\n",
        "print(f'Execution time of the moving average model is: {execution_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFtc1aLoVG7U"
      },
      "outputs": [],
      "source": [
        "plot_forecast_vs_actual(y_test, ma_predictions, title=\"Forecasted vs Actual Values\", num_samples=200)\n",
        "\n",
        "plot_residuals(y_test, ma_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5d0-TLNzUJ_"
      },
      "source": [
        "### **Seasonal Naïve**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6Tfa_wfzxTM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SeasonalNaiveForecast:\n",
        "  def __init__(self, season_length):\n",
        "      self.return_value = None\n",
        "      self.season_length = season_length\n",
        "\n",
        "  def fit(self, series):\n",
        "      self.return_value = series.iloc[-self.season_length:].values\n",
        "\n",
        "  def predict(self, n_periods=1):\n",
        "      if self.return_value is None:\n",
        "          raise ValueError(\"Model is not fitted yet. Call fit() before predict().\")\n",
        "      predictions = []\n",
        "      for i in range(n_periods):\n",
        "          predictions.append(self.return_value[i % self.season_length])\n",
        "      return np.array(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2XrHT4eK3U_"
      },
      "outputs": [],
      "source": [
        "def pred_naive_seasonal_grid_search(y_train, y_test, season_lengths = [1, 3, 5, 7, 10, 14, 21, 30]):\n",
        "  best_season_length = None\n",
        "  best_score = float('inf')\n",
        "\n",
        "  for season_length in season_lengths:\n",
        "    seasonal_naive_model = SeasonalNaiveForecast(season_length=season_length)\n",
        "    seasonal_naive_model.fit(y_train)\n",
        "    predictions = seasonal_naive_model.predict(n_periods=len(y_test))\n",
        "\n",
        "    score = mean_squared_error(y_test, predictions)\n",
        "    if score < best_score:\n",
        "        best_score = score\n",
        "        best_season_length = season_length\n",
        "\n",
        "  start_time = time.time()\n",
        "  final_model = SeasonalNaiveForecast(season_length=best_season_length)\n",
        "  final_model.fit(y_train)\n",
        "  final_predictions = final_model.predict(n_periods=len(y_test))\n",
        "  end_time = time.time()\n",
        "\n",
        "  name = f\"Seasonal Naive (Best Season Length: {best_season_length})\"\n",
        "  execution_time = end_time - start_time\n",
        "  return final_predictions, name, execution_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abNLz6dSVlx5"
      },
      "outputs": [],
      "source": [
        "y_train, y_test = train_test_split(y_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make predictions using the seasonal naive model\n",
        "seasonal_naive_preds, model_name, execution_time = pred_naive_seasonal_grid_search(y_train, y_test)\n",
        "\n",
        "evaluate_metrics_naive(y_test, seasonal_naive_preds)\n",
        "print(f'Execution time of the naive model is: {execution_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDDl7VKiVvDJ"
      },
      "outputs": [],
      "source": [
        "plot_forecast_vs_actual(y_test, seasonal_naive_preds, title=\"Forecasted vs Actual Values\", num_samples=200)\n",
        "\n",
        "plot_residuals(y_test, seasonal_naive_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulOpBYsz0W4"
      },
      "source": [
        "## **6.3 Machine Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsjlv6QpVws"
      },
      "source": [
        "###  **XGBOOST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5f3IfNar3nk"
      },
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X_cleaned)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 2. Train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=500,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# 3. Model Evaluation\n",
        "y_pred_train = xgb_model.predict(X_train)\n",
        "y_pred_test = xgb_model.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Metrics\n",
        "evaluate_metrics(y_train, y_pred_train, y_test, y_pred_test)\n",
        "\n",
        "# Feature Importance\n",
        "importance = xgb_model.feature_importances_\n",
        "\n",
        "min_length = min(len(new_features), len(importance))\n",
        "feature_importance = pd.DataFrame({'Feature': new_features[:min_length], 'Importance': importance[:min_length]})\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))\n",
        "execution_time = end_time - start_time\n",
        "print(f\"XGBoost execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzFOvqfcD8TB"
      },
      "outputs": [],
      "source": [
        "#Real vs predicted\n",
        "plot_forecast_vs_actual(y_test, y_pred_test, title=\"Forecasted vs Actual Values (First 200 Samples)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NJLEPv4DkKZ"
      },
      "outputs": [],
      "source": [
        "# Residuals\n",
        "plot_residuals(y_test, y_pred_test, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ksvrMOvB7w"
      },
      "source": [
        "### **Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovmgOqf2seJI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# Handle missing values (if any)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "\n",
        "# Handle outliers using the function and print the count of outliers replaced\n",
        "\n",
        "# # Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_cleaned)\n",
        "\n",
        "# # Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_cleaned, test_size=0.2, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=500,  # Number of trees\n",
        "    max_depth=None,    # Expand until all leaves are pure\n",
        "    random_state=42,\n",
        "    n_jobs=-1          # Utilize all processors\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = rf_model.predict(X_train)\n",
        "y_pred_test = rf_model.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_metrics(y_train, y_pred_train, y_test, y_pred_test)\n",
        "\n",
        "min_length = min(len(new_features), len(rf_model.feature_importances_))\n",
        "feature_importance = pd.DataFrame({'Feature': new_features[:min_length], 'Importance': rf_model.feature_importances_[:min_length]})\n",
        "print(feature_importance.sort_values(by='Importance', ascending=False))\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Random Forest execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlPtp0rqw9lU"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual(y_test, y_pred_test, title=\"Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C8aCEuhExB4"
      },
      "outputs": [],
      "source": [
        "# Plot residuals for the test set\n",
        "plot_residuals(y_test, y_pred_test, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_K2F1B71Zbt"
      },
      "source": [
        "### **Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL4v6zSdzOOc"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error,mean_absolute_percentage_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Define parameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 10, 20],\n",
        "    'min_samples_leaf': [1, 5, 10],\n",
        "    'max_features': [None, 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and parameters\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Fit the best model\n",
        "best_dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = best_dt_model.predict(X_train)\n",
        "y_pred_test = best_dt_model.predict(X_test)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_metrics(y_train, y_pred_train, y_test, y_pred_test)\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Decision Tree execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hx15VXDGDhf"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual(y_test, y_pred_test, title=\"Forecasted vs Actual Values (Test Set)\")\n",
        "\n",
        "# Plot residuals for the test set\n",
        "plot_residuals(y_test, y_pred_test, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpvfwQW40MdM"
      },
      "source": [
        "## **6.4 Deep Learning Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ure--Kqy3riJ"
      },
      "source": [
        "### **LSTM (Long Short-Term Memory)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW7-lU9331ke"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class for LSTM\n",
        "class EnergyDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_seq = self.X[index:index + self.seq_length]\n",
        "        y_seq = self.y[index + self.seq_length - 1]\n",
        "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Train-test split\n",
        "seq_length = 24\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "train_dataset = EnergyDataset(X_train, y_train, seq_length)\n",
        "test_dataset = EnergyDataset(X_test, y_test, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model parameters\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "# Initialize model\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        y_pred.extend(outputs.cpu().numpy())\n",
        "        y_true.extend(y_batch.cpu().numpy())\n",
        "\n",
        "# Inverse scaling\n",
        "y_pred = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
        "y_true = scaler.inverse_transform(np.array(y_true).reshape(-1, 1))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, rmse, mae, mape, r2\n",
        "\n",
        "mse, rmse, mae, mape, r2 = calculate_metrics(y_true, y_pred)\n",
        "print(f'RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.4f}, R^2: {r2:.4f}')\n",
        "execution_time = end_time - start_time\n",
        "print(f\"LSTM execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPqEXHOqHT_P"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual_dl(y_true, y_pred, title=\"Forecasted vs Actual Values (Test Set)\")\n",
        "\n",
        "# Plot residuals for the test set\n",
        "plot_residuals(y_true, y_pred, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBsvgEjc3vtC"
      },
      "source": [
        "### **GRU (Gated Recurrent Unit)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y-Log-_319s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class for GRU\n",
        "class EnergyDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_seq = self.X[index:index + self.seq_length]\n",
        "        y_seq = self.y[index + self.seq_length - 1]\n",
        "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "# GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Train-test split\n",
        "seq_length = 24\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "train_dataset = EnergyDataset(X_train, y_train, seq_length)\n",
        "test_dataset = EnergyDataset(X_test, y_test, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model parameters\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "output_size = 1\n",
        "\n",
        "# Initialize model\n",
        "model = GRUModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        y_pred.extend(outputs.cpu().numpy())\n",
        "        y_true.extend(y_batch.cpu().numpy())\n",
        "\n",
        "# Inverse scaling\n",
        "y_pred = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
        "y_true = scaler.inverse_transform(np.array(y_true).reshape(-1, 1))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, rmse, mae, mape, r2\n",
        "\n",
        "mse, rmse, mae, mape, r2 = calculate_metrics(y_true, y_pred)\n",
        "print(f'RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.4f}, R^2: {r2:.4f}')\n",
        "execution_time = end_time - start_time\n",
        "print(f\"GRU execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCCwzaCIRNuI"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual_dl(y_true, y_pred, title=\"Forecasted vs Actual Values (Test Set)\")\n",
        "\n",
        "# Plot residuals for the test set\n",
        "plot_residuals(y_true, y_pred, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahn5MVwoAobF"
      },
      "source": [
        "### **CNNs for Time Series (TCN)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpkGykBIeNG8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class for TCN model\n",
        "class EnergyDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_seq = self.X[index:index + self.seq_length]\n",
        "        y_seq = self.y[index + self.seq_length - 1]\n",
        "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "# TCN Model with dilated convolutions\n",
        "class TCNModel(nn.Module):\n",
        "    def __init__(self, input_size, seq_length, num_filters, kernel_size, output_size, num_layers=5):\n",
        "        super(TCNModel, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "\n",
        "        # Add TCN layers with dilated convolutions\n",
        "        for i in range(num_layers):\n",
        "            dilation = 2 ** i\n",
        "            self.conv_layers.append(\n",
        "                nn.Conv1d(in_channels=input_size if i == 0 else num_filters,\n",
        "                          out_channels=num_filters,\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding=dilation,\n",
        "                          dilation=dilation)\n",
        "            )\n",
        "            # Batch normalization and ReLU activation\n",
        "            self.conv_layers.append(nn.BatchNorm1d(num_filters))\n",
        "            self.conv_layers.append(nn.ReLU())\n",
        "\n",
        "        self.fc = nn.Linear(num_filters, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "\n",
        "        x = x.mean(dim=2)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Train-test split\n",
        "seq_length = 24\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "train_dataset = EnergyDataset(X_train, y_train, seq_length)\n",
        "test_dataset = EnergyDataset(X_test, y_test, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model parameters\n",
        "input_size = X.shape[1]\n",
        "num_filters = 64\n",
        "kernel_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Initialize model\n",
        "model = TCNModel(input_size, seq_length, num_filters, kernel_size, output_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        y_pred.extend(outputs.cpu().numpy())\n",
        "        y_true.extend(y_batch.cpu().numpy())\n",
        "\n",
        "# Inverse scaling\n",
        "y_pred = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
        "y_true = scaler.inverse_transform(np.array(y_true).reshape(-1, 1))\n",
        "\n",
        "# Metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, rmse, mae, mape, r2\n",
        "\n",
        "mse, rmse, mae, mape, r2 = calculate_metrics(y_true, y_pred)\n",
        "print(f'RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.4f}, R^2: {r2:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YvV7xpkR267"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual_dl(y_true, y_pred, title=\"Forecasted vs Actual Values (Test Set)\")\n",
        "\n",
        "# Plot residuals for the test set\n",
        "plot_residuals(y_true, y_pred, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCMZffMDAr5q"
      },
      "source": [
        "## **6.5 Transformer Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmsOelKTArKQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset class for Transformer\n",
        "class EnergyDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_seq = self.X[index:index + self.seq_length]\n",
        "        y_seq = self.y[index + self.seq_length - 1]\n",
        "        return torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "# Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_size, d_model, nhead, num_encoder_layers, dim_feedforward, seq_length, output_size, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, d_model))\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout),\n",
        "            num_encoder_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.positional_encoding\n",
        "        x = self.encoder(x.permute(1, 0, 2))  # Transformer expects (seq_length, batch_size, d_model)\n",
        "        x = self.fc(x[-1])  # Use the last time step\n",
        "        return x\n",
        "\n",
        "# Preprocessing\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))\n",
        "\n",
        "# Train-test split\n",
        "seq_length = 24\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "train_dataset = EnergyDataset(X_train, y_train, seq_length)\n",
        "test_dataset = EnergyDataset(X_test, y_test, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model parameters\n",
        "input_size = X.shape[1]\n",
        "d_model = 64\n",
        "nhead = 4\n",
        "num_encoder_layers = 3\n",
        "dim_feedforward = 128\n",
        "output_size = 1\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize model\n",
        "model = TransformerModel(input_size, d_model, nhead, num_encoder_layers, dim_feedforward, seq_length, output_size, dropout).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        y_pred.extend(outputs.cpu().numpy())\n",
        "        y_true.extend(y_batch.cpu().numpy())\n",
        "\n",
        "# Inverse scaling\n",
        "y_pred = scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
        "y_true = scaler.inverse_transform(np.array(y_true).reshape(-1, 1))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Metrics\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, rmse, mae, mape, r2\n",
        "\n",
        "mse, rmse, mae, mape, r2 = calculate_metrics(y_true, y_pred)\n",
        "print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.4f}, R^2: {r2:.4f}')\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Transformer execution time: {execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQuM6MyES4rU"
      },
      "outputs": [],
      "source": [
        "# Plot forecasted vs actual values for the test set\n",
        "plot_forecast_vs_actual_dl(y_true, y_pred, title=\"Forecasted vs Actual Values (Test Set)\")\n",
        "\n",
        "# Plot residuals for the test set\n",
        "plot_residuals(y_true, y_pred, title=\"Residuals of Forecasted vs Actual Values (Test Set)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Zt-UqndWQr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
